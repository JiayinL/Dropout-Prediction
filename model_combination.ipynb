{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel,delayed\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_video_act_train_2_encoded_course_vec = pd.read_csv('user_video_act_train_2_encoded_course_vec.csv',converters={'label_list': eval, 'course_vecs_CNN':eval, 'course_vecs_LR':eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>course_list</th>\n",
       "      <th>label_list</th>\n",
       "      <th>course_ids</th>\n",
       "      <th>video_ids</th>\n",
       "      <th>watching_counts</th>\n",
       "      <th>video_durations</th>\n",
       "      <th>local_watching_times</th>\n",
       "      <th>video_progress_times</th>\n",
       "      <th>video_start_times</th>\n",
       "      <th>video_end_times</th>\n",
       "      <th>local_start_times</th>\n",
       "      <th>local_end_times</th>\n",
       "      <th>courseListIDs</th>\n",
       "      <th>course_vecs_LR</th>\n",
       "      <th>course_vecs_CNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'course_id': 'C_course-v1:TsinghuaX+20430064...</td>\n",
       "      <td>['C_course-v1:TsinghuaX+20430064X+sp', 'C_cour...</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[455, 492, 379, 455, 492, 492, 496, 455, 496, ...</td>\n",
       "      <td>[837, 1889, 1903, 2406, 2660, 4455, 4682, 4788...</td>\n",
       "      <td>[1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 4, 1, ...</td>\n",
       "      <td>[563.0, 166.0, 415.0, 173.0, 63.0, 262.0, 1088...</td>\n",
       "      <td>[563, 166, 415, 174, 19, 262, 2097, 254, 442, ...</td>\n",
       "      <td>[563.0, 166.0, 414.8099975585936, 173.0, 18.81...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[563.0, 166.0, 415.0, 173.0, 18.81999969482422...</td>\n",
       "      <td>['2017-07-24 13:26:56', '2017-07-06 14:47:00',...</td>\n",
       "      <td>['2017-07-24 13:36:19', '2017-07-06 14:49:46',...</td>\n",
       "      <td>[455 492 379 496 400]</td>\n",
       "      <td>[[455, -0.41622926448634734, -0.01951928858061...</td>\n",
       "      <td>[[[455, 837, -0.4624306896892794, 0.1235288815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'course_id': 'C_course-v1:TsinghuaX+30640014...</td>\n",
       "      <td>['C_course-v1:TsinghuaX+30640014X+sp', 'C_cour...</td>\n",
       "      <td>[0, 1, 1, 1]</td>\n",
       "      <td>[481, 403, 404, 400, 481, 481, 403, 404, 481, ...</td>\n",
       "      <td>[3828, 4091, 4091, 5207, 5331, 6638, 8825, 882...</td>\n",
       "      <td>[33, 3, 3, 22, 2, 1, 1, 1, 1, 1, 5, 6, 16, 12,...</td>\n",
       "      <td>[748.0, 93.0, 93.0, 1176.0, 79.0, 98.0, 273.0,...</td>\n",
       "      <td>[825, 24, 24, 1429, 39, 99, 43, 43, 79, 11, 30...</td>\n",
       "      <td>[874.1998138427736, 24.5, 24.5, 2721.459894180...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 19.1200008392334, 41.310001373...</td>\n",
       "      <td>[748.0, 16.229999542236328, 16.229999542236328...</td>\n",
       "      <td>['2018-03-18 13:40:31', '2019-01-31 11:54:47',...</td>\n",
       "      <td>['2018-03-18 15:14:03', '2019-01-31 14:48:29',...</td>\n",
       "      <td>[481 403 404 400]</td>\n",
       "      <td>[[481, -0.5597614352282316, -0.189907587438736...</td>\n",
       "      <td>[[[481, 3828, 2.7281347101059863, 0.6644491379...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            activity  \\\n",
       "0  [{'course_id': 'C_course-v1:TsinghuaX+20430064...   \n",
       "1  [{'course_id': 'C_course-v1:TsinghuaX+30640014...   \n",
       "\n",
       "                                         course_list       label_list  \\\n",
       "0  ['C_course-v1:TsinghuaX+20430064X+sp', 'C_cour...  [1, 1, 0, 0, 0]   \n",
       "1  ['C_course-v1:TsinghuaX+30640014X+sp', 'C_cour...     [0, 1, 1, 1]   \n",
       "\n",
       "                                          course_ids  \\\n",
       "0  [455, 492, 379, 455, 492, 492, 496, 455, 496, ...   \n",
       "1  [481, 403, 404, 400, 481, 481, 403, 404, 481, ...   \n",
       "\n",
       "                                           video_ids  \\\n",
       "0  [837, 1889, 1903, 2406, 2660, 4455, 4682, 4788...   \n",
       "1  [3828, 4091, 4091, 5207, 5331, 6638, 8825, 882...   \n",
       "\n",
       "                                     watching_counts  \\\n",
       "0  [1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 4, 1, ...   \n",
       "1  [33, 3, 3, 22, 2, 1, 1, 1, 1, 1, 5, 6, 16, 12,...   \n",
       "\n",
       "                                     video_durations  \\\n",
       "0  [563.0, 166.0, 415.0, 173.0, 63.0, 262.0, 1088...   \n",
       "1  [748.0, 93.0, 93.0, 1176.0, 79.0, 98.0, 273.0,...   \n",
       "\n",
       "                                local_watching_times  \\\n",
       "0  [563, 166, 415, 174, 19, 262, 2097, 254, 442, ...   \n",
       "1  [825, 24, 24, 1429, 39, 99, 43, 43, 79, 11, 30...   \n",
       "\n",
       "                                video_progress_times  \\\n",
       "0  [563.0, 166.0, 414.8099975585936, 173.0, 18.81...   \n",
       "1  [874.1998138427736, 24.5, 24.5, 2721.459894180...   \n",
       "\n",
       "                                   video_start_times  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 19.1200008392334, 41.310001373...   \n",
       "\n",
       "                                     video_end_times  \\\n",
       "0  [563.0, 166.0, 415.0, 173.0, 18.81999969482422...   \n",
       "1  [748.0, 16.229999542236328, 16.229999542236328...   \n",
       "\n",
       "                                   local_start_times  \\\n",
       "0  ['2017-07-24 13:26:56', '2017-07-06 14:47:00',...   \n",
       "1  ['2018-03-18 13:40:31', '2019-01-31 11:54:47',...   \n",
       "\n",
       "                                     local_end_times          courseListIDs  \\\n",
       "0  ['2017-07-24 13:36:19', '2017-07-06 14:49:46',...  [455 492 379 496 400]   \n",
       "1  ['2018-03-18 15:14:03', '2019-01-31 14:48:29',...      [481 403 404 400]   \n",
       "\n",
       "                                      course_vecs_LR  \\\n",
       "0  [[455, -0.41622926448634734, -0.01951928858061...   \n",
       "1  [[481, -0.5597614352282316, -0.189907587438736...   \n",
       "\n",
       "                                     course_vecs_CNN  \n",
       "0  [[[455, 837, -0.4624306896892794, 0.1235288815...  \n",
       "1  [[[481, 3828, 2.7281347101059863, 0.6644491379...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_video_act_train_2_encoded_course_vec.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_transform(t):\n",
    "    # 先转换为时间数组\n",
    "    timeArray = time.strptime(t, \"%Y-%m-%d %H:%M:%S\")\n",
    "    # 转换为时间戳\n",
    "    timeStamp = int(time.mktime(timeArray))\n",
    "    return timeStamp\n",
    "def Z_score(mean_, std_,x):\n",
    "    return (x-mean_)/std_\n",
    "def max_mean_std(data):\n",
    "    return np.max(data), np.mean(data), np.std(data)\n",
    "def calculate_acc(predictions, truth):\n",
    "    hit = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == truth[i]:\n",
    "            hit = hit +1\n",
    "    return hit/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_courses = 706+1\n",
    "course_emb_size = 5\n",
    "nb_videos = 38181+1\n",
    "video_emb_size = 15\n",
    "feature_size1 = course_emb_size + 42\n",
    "sequence_len = 70\n",
    "feature_size2 = course_emb_size + video_emb_size + 13\n",
    "num_out_channel = 32\n",
    "kernel_size = [3,4,5]\n",
    "output_size = 32\n",
    "hidden_dim = 64\n",
    "num_of_lstm_layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.fc = nn.Linear(feature_size1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()        \n",
    "\n",
    "    def forward(self, course_id, continues):\n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "       \n",
    "        x = torch.cat([emb,continues], 1)\n",
    "\n",
    "        result = self.sigmoid(self.fc(x))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        \n",
    "#         self.bi_gru = nn.GRU(input_size = feature_size, hidden_size = hidden_dim, num_layers=num_of_lstm_layer,  bidirectional=False)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(feature_size2*sequence_len, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, course_id, video_id, continues,b_size):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "        \n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        \n",
    "        input_x = x.view(b_size,-1)\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(input_x))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        \n",
    "        final_out = self.fc3(info_fusion)\n",
    "\n",
    "        result = self.sigmoid(final_out)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "        self.bilstm = nn.LSTM(input_size = feature_size2, hidden_size = hidden_dim , num_layers=num_of_lstm_layer,  bidirectional=False)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)  #  max_sen_len * Batch_size * feature_dim\n",
    "       \n",
    "        bilstm_out, _ = self.bilstm(x)\n",
    "        output = bilstm_out[-1]\n",
    "\n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))   \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        result = self.sigmoid(final_out)\n",
    "        \n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.bilstm = nn.LSTM(feature_size2, hidden_dim//2 , num_layers=num_of_lstm_layer,  bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "        \n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        x = x.permute(1, 0, 2)  #  max_sen_len * Batch_size * feature_dim\n",
    "       \n",
    "        bilstm_out, _ = self.bilstm(x)\n",
    "        output = bilstm_out[-1]\n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        final_out = self.fc3(info_fusion)\n",
    "        result = self.sigmoid(final_out)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        \n",
    "        self.bi_gru = nn.GRU(input_size = feature_size2, hidden_size = hidden_dim, num_layers=num_of_lstm_layer,  bidirectional=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "        \n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        x = x.permute(1, 0, 2)  #  max_sen_len * Batch_size * feature_dim\n",
    "       \n",
    "        bigru_out, _ = self.bi_gru(x)\n",
    "        output = bigru_out[-1]\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        \n",
    "        final_out = self.fc3(info_fusion)\n",
    "\n",
    "        result = self.sigmoid(final_out)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BiGRU, self).__init__()\n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        \n",
    "        self.bi_gru = nn.GRU(input_size = feature_size2, hidden_size = hidden_dim // 2 , num_layers=num_of_lstm_layer,  bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        x = x.permute(1, 0, 2)  #  max_sen_len * Batch_size * feature_dim\n",
    "       \n",
    "        bigru_out, _ = self.bi_gru(x)\n",
    "        output = bigru_out[-1]\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))    \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        result = self.sigmoid(final_out)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU_MultiAttent(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BiGRU_MultiAttent, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax( dim=1)\n",
    "        \n",
    "        self.bi_gru = nn.GRU(input_size = feature_size2, hidden_size = hidden_dim , num_layers=num_of_lstm_layer,  bidirectional=True)\n",
    "        \n",
    "        self.self_attention = nn.MultiheadAttention(feature_size2, num_heads = 11)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "        \n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        x = x.permute(1, 0, 2)  #  max_sen_len * Batch_size * feature_dim\n",
    "        \n",
    "        #MultiHead Attention \n",
    "        attn_output, attn_output_weights = self.self_attention(x, x, x)\n",
    "        \n",
    "        #GRU\n",
    "        bigru_out, _ = self.bi_gru(attn_output)\n",
    "        output = bigru_out[-1]\n",
    "        \n",
    "      \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        final_out = self.fc3(info_fusion)\n",
    "        result = self.sigmoid(final_out)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[0])\n",
    "        self.maxpool1 = nn.MaxPool1d(sequence_len - kernel_size[0] + 1) \n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[1])\n",
    "        self.maxpool2 = nn.MaxPool1d(sequence_len - kernel_size[1] + 1) \n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[2])\n",
    "        self.maxpool3 = nn.MaxPool1d(sequence_len - kernel_size[2] + 1)\n",
    "        self.fc1 = nn.Linear(num_out_channel*len(kernel_size), 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "#         self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "        x = x.permute(0, 2, 1)  # Batch_size * (feature_dim) * max_sen_len\n",
    "\n",
    "        x1 = self.conv1(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x1 = self.ReLU_activation(x1)\n",
    "        x1 = self.maxpool1(x1)\n",
    "        x1 = x1.squeeze(2) \n",
    "        \n",
    "        x2 = self.conv2(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x2 = self.ReLU_activation(x2)\n",
    "        x2 = self.maxpool2(x2)\n",
    "        x2 = x2.squeeze(2)\n",
    "        \n",
    "        x3 = self.conv3(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x3 = self.ReLU_activation(x3)\n",
    "        x3 = self.maxpool3(x3)\n",
    "        x3 = x3.squeeze(2)\n",
    "\n",
    "        all_out = torch.cat((x1, x2, x3), dim=1)\n",
    "#         print(all_out.shape)\n",
    "        info_fusion = self.tanh_activation(self.fc1(all_out))       \n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "#         result = self.softmax(final_out)\n",
    "        result = self.sigmoid(final_out)\n",
    "\n",
    "        return result  # 返回 softmax 的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh for conv results, lppool poolling used\n",
    "class TextCNN_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN_v2, self).__init__()\n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[0])\n",
    "        self.maxpool1 = nn.MaxPool1d(sequence_len - kernel_size[0] + 1)\n",
    "        self.lppool1 =  nn.LPPool1d(2, sequence_len - kernel_size[0] + 1)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[1])\n",
    "        self.maxpool2 = nn.MaxPool1d(sequence_len - kernel_size[1] + 1)\n",
    "        self.lppool2 =  nn.LPPool1d(2, sequence_len - kernel_size[1] + 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[2])\n",
    "        self.maxpool3 = nn.MaxPool1d(sequence_len - kernel_size[2] + 1)\n",
    "        self.lppool3 =  nn.LPPool1d(2, sequence_len - kernel_size[2] + 1)\n",
    "#         self.conv4 = nn.Conv1d(in_channels=feature_size,out_channels=num_out_channel,kernel_size=kernel_size[3])\n",
    "#         self.maxpool4 = nn.MaxPool1d(sequence_len - kernel_size[3] + 1)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_out_channel*len(kernel_size), 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "#         self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "#         print(x.shape)\n",
    "        x = x.permute(0, 2, 1)  # Batch_size * (feature_dim) * max_sen_len\n",
    "#         print('input ',x.shape)\n",
    "        x1 = self.conv1(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x1.shape)\n",
    "        x1 = self.sigmoid(x1)\n",
    "#         print('actication result: ',x1.shape)\n",
    "#         x1 = self.maxpool1(x1)\n",
    "        x1 = self.lppool1(x1)\n",
    "#         print('maxpool: ',x1.shape)\n",
    "        x1 = x1.squeeze(2)\n",
    "#         print('final x1: ',x1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x2.shape)\n",
    "        x2 = self.sigmoid(x2)\n",
    "#         print('actication result: ',x2.shape)\n",
    "#         x2 = self.maxpool2(x2)\n",
    "        x2 = self.lppool2(x2)\n",
    "#         print('maxpool: ',x2.shape)\n",
    "        x2 = x2.squeeze(2)\n",
    "#         print('final x2: ',x2.shape)\n",
    "        \n",
    "        \n",
    "        x3 = self.conv3(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x3.shape)\n",
    "        x3 = self.sigmoid(x3)\n",
    "#         print('actication result: ',x3.shape)\n",
    "#         x3 = self.maxpool3(x3)\n",
    "        x3 = self.lppool3(x3)\n",
    "#         print('maxpool: ',x3.shape)\n",
    "        x3 = x3.squeeze(2)\n",
    " \n",
    "        all_out = torch.cat((x1, x2, x3), dim=1)\n",
    "#         print(all_out.shape)\n",
    "        info_fusion = self.tanh_activation(self.fc1(all_out))\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        \n",
    "        final_out = self.fc3(info_fusion)\n",
    "#         result = self.softmax(final_out)\n",
    "        result = self.sigmoid(final_out)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        return result  # 返回 softmax 的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN_MultiHAtten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN_MultiHAtten, self).__init__()\n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[0])\n",
    "        self.maxpool1 = nn.MaxPool1d(sequence_len - kernel_size[0] + 1)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[1])\n",
    "        self.maxpool2 = nn.MaxPool1d(sequence_len - kernel_size[1] + 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[2])\n",
    "        self.maxpool3 = nn.MaxPool1d(sequence_len - kernel_size[2] + 1)\n",
    "        \n",
    "#         self.conv4 = nn.Conv1d(in_channels=feature_size,out_channels=num_out_channel,kernel_size=kernel_size[3])\n",
    "#         self.maxpool4 = nn.MaxPool1d(sequence_len - kernel_size[3] + 1)\n",
    "        \n",
    "        \n",
    "        self.self_attention = nn.MultiheadAttention(num_out_channel*len(kernel_size), num_heads = 6)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_out_channel*len(kernel_size), 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "#         self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "#         print(x.shape)\n",
    "        x = x.permute(0, 2, 1)  # Batch_size * (feature_dim) * max_sen_len\n",
    "#         print('input ',x.shape)\n",
    "        x1 = self.conv1(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x1.shape)\n",
    "        x1 = self.ReLU_activation(x1)\n",
    "#         print('actication result: ',x1.shape)\n",
    "        x1 = self.maxpool1(x1)\n",
    "#         print('maxpool: ',x1.shape)\n",
    "        x1 = x1.squeeze(2)\n",
    "#         print('final x1: ',x1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x2.shape)\n",
    "        x2 = self.ReLU_activation(x2)\n",
    "#         print('actication result: ',x2.shape)\n",
    "        x2 = self.maxpool2(x2)\n",
    "#         print('maxpool: ',x2.shape)\n",
    "        x2 = x2.squeeze(2)\n",
    "#         print('final x2: ',x2.shape)\n",
    "        \n",
    "        \n",
    "        x3 = self.conv3(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x3.shape)\n",
    "        x3 = self.ReLU_activation(x3)\n",
    "#         print('actication result: ',x3.shape)\n",
    "        x3 = self.maxpool3(x3)\n",
    "#         print('maxpool: ',x3.shape)\n",
    "        x3 = x3.squeeze(2)\n",
    "#         print('final x3: ',x3.shape)\n",
    "\n",
    "#         x4 = self.conv4(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "# #         print('convolution result: ',x3.shape)\n",
    "#         x4 = self.ReLU_activation(x4)\n",
    "# #         print('actication result: ',x3.shape)\n",
    "#         x4 = self.maxpool4(x4)\n",
    "# #         print('maxpool: ',x3.shape)\n",
    "#         x4 = x4.squeeze(2)\n",
    "        \n",
    "        \n",
    "#         x2 = self.conv2(x)#.squeeze(2)\n",
    "#         x3 = self.conv3(x)#.squeeze(2)\n",
    "        all_out = torch.cat((x1, x2, x3), dim=1)\n",
    "#         print('all_out: ',all_out.shape)\n",
    "        \n",
    "        all_out = all_out.unsqueeze(2)\n",
    "#         print('all_out: ',all_out.shape)\n",
    "        all_out = all_out.permute(2, 0, 1) \n",
    "#         print('all_out: ',all_out.shape)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.self_attention(all_out, all_out, all_out)\n",
    "#         print('attn_output: ',attn_output.shape)\n",
    "        attn_output = attn_output.permute(1, 2, 0)\n",
    "#         print('attn_output2: ',attn_output.shape)\n",
    "        \n",
    "        attn_output = attn_output.squeeze(2)\n",
    "#         print('attn_output3: ',attn_output.shape)\n",
    "        info_fusion = self.tanh_activation(self.fc1(attn_output))\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        \n",
    "        final_out = self.fc3(info_fusion)\n",
    "#         result = self.softmax(final_out)\n",
    "        result = self.sigmoid(final_out)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        return result  # 返回 softmax 的结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = [2,3,4,5]\n",
    "class TextCNN_4conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN_4conv, self).__init__()\n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[0])\n",
    "        self.maxpool1 = nn.MaxPool1d(sequence_len - kernel_size[0] + 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[1])\n",
    "        self.maxpool2 = nn.MaxPool1d(sequence_len - kernel_size[1] + 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[2])\n",
    "        self.maxpool3 = nn.MaxPool1d(sequence_len - kernel_size[2] + 1)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[3])\n",
    "        self.maxpool4 = nn.MaxPool1d(sequence_len - kernel_size[3] + 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_out_channel*len(kernel_size), 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "#         self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, course_id, video_id, continues):\n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1 = self.course_embedding(course_id) # (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(video_id)\n",
    "\n",
    "        x = torch.cat([emb1,emb2,continues], 2)\n",
    "#         print(x.shape)\n",
    "        x = x.permute(0, 2, 1)  # Batch_size * (feature_dim) * max_sen_len\n",
    "#         print('input ',x.shape)\n",
    "        x1 = self.conv1(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x1.shape)\n",
    "        x1 = self.ReLU_activation(x1)\n",
    "        x1 = self.maxpool1(x1)\n",
    "        x1 = x1.squeeze(2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x2 = self.conv2(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x2.shape)\n",
    "        x2 = self.ReLU_activation(x2)\n",
    "#         print('actication result: ',x2.shape)\n",
    "        x2 = self.maxpool2(x2)\n",
    "#         print('maxpool: ',x2.shape)\n",
    "        x2 = x2.squeeze(2)\n",
    "#         print('final x2: ',x2.shape)\n",
    "        \n",
    "        \n",
    "        x3 = self.conv3(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x3.shape)\n",
    "        x3 = self.ReLU_activation(x3)\n",
    "#         print('actication result: ',x3.shape)\n",
    "        x3 = self.maxpool3(x3)\n",
    "#         print('maxpool: ',x3.shape)\n",
    "        x3 = x3.squeeze(2)\n",
    "#         print('final x3: ',x3.shape)\n",
    "\n",
    "        x4 = self.conv4(x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         print('convolution result: ',x3.shape)\n",
    "        x4 = self.ReLU_activation(x4)\n",
    "#         print('actication result: ',x3.shape)\n",
    "        x4 = self.maxpool4(x4)\n",
    "#         print('maxpool: ',x3.shape)\n",
    "        x4 = x4.squeeze(2)\n",
    "        \n",
    "        \n",
    "#         x2 = self.conv2(x)#.squeeze(2)\n",
    "#         x3 = self.conv3(x)#.squeeze(2)\n",
    "        all_out = torch.cat((x1, x2, x3,x4), dim=1)\n",
    "#         print(all_out.shape)\n",
    "        info_fusion = self.tanh_activation(self.fc1(all_out))\n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        \n",
    "        final_out = self.fc3(info_fusion)\n",
    "#         result = self.softmax(final_out)\n",
    "        result = self.sigmoid(final_out)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        return result  # 返回 softmax 的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_CNN, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "                \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[0])\n",
    "        self.maxpool1 = nn.MaxPool1d(sequence_len - kernel_size[0] + 1)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[1])\n",
    "        self.maxpool2 = nn.MaxPool1d(sequence_len - kernel_size[1] + 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[2])\n",
    "        self.maxpool3 = nn.MaxPool1d(sequence_len - kernel_size[2] + 1)\n",
    "    \n",
    "        \n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_out_channel*len(kernel_size), 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        #CNN part\n",
    "\n",
    "        CNN_x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        CNN_x = CNN_x.permute(0, 2, 1)  # Batch_size * (feature_dim) * max_sen_len\n",
    "        x1 = self.conv1(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x1 = self.ReLU_activation(x1)\n",
    "        x1 = self.maxpool1(x1)\n",
    "        x1 = x1.squeeze(2)\n",
    "#         print('final x1: ',x1.shape)\n",
    "       \n",
    "        x2 = self.conv2(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x2 = self.ReLU_activation(x2)\n",
    "        x2 = self.maxpool2(x2)\n",
    "        x2 = x2.squeeze(2)\n",
    "        \n",
    "        x3 = self.conv3(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x3 = self.ReLU_activation(x3)\n",
    "        x3 = self.maxpool3(x3)\n",
    "        x3 = x3.squeeze(2)\n",
    "     \n",
    "        \n",
    "        all_out = torch.cat((x1, x2, x3), dim=1)\n",
    "        info_fusion = self.tanh_activation(self.fc1(all_out))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        CNN_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        #combine two result\n",
    "        final_input = torch.cat((LR_result, CNN_result), dim=1)\n",
    "#         print(final_input.shape)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))        \n",
    "        \n",
    "        return result,LR_result,CNN_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_CNN_atten(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_CNN_atten, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "                \n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[0])\n",
    "        self.maxpool1 = nn.MaxPool1d(sequence_len - kernel_size[0] + 1)\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[1])\n",
    "        self.maxpool2 = nn.MaxPool1d(sequence_len - kernel_size[1] + 1)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=feature_size2,out_channels=num_out_channel,kernel_size=kernel_size[2])\n",
    "        self.maxpool3 = nn.MaxPool1d(sequence_len - kernel_size[2] + 1)\n",
    "        \n",
    "#         self.conv4 = nn.Conv1d(in_channels=feature_size,out_channels=num_out_channel,kernel_size=kernel_size[3])\n",
    "#         self.maxpool4 = nn.MaxPool1d(sequence_len - kernel_size[3] + 1)\n",
    "        self.self_attention = nn.MultiheadAttention(num_out_channel*len(kernel_size), num_heads = 6)\n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_out_channel*len(kernel_size), 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        #CNN part\n",
    "#         print('emb1_CNN:',emb1_CNN.shape)\n",
    "#         print('emb2:',emb2.shape)\n",
    "#         print('continuesfeature2:',continuesfeature2.shape)\n",
    "        \n",
    "        CNN_x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        CNN_x = CNN_x.permute(0, 2, 1)  # Batch_size * (feature_dim) * max_sen_len\n",
    "        x1 = self.conv1(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x1 = self.ReLU_activation(x1)\n",
    "        x1 = self.maxpool1(x1)\n",
    "        x1 = x1.squeeze(2)\n",
    "#         print('final x1: ',x1.shape)\n",
    "       \n",
    "        x2 = self.conv2(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x2 = self.ReLU_activation(x2)\n",
    "        x2 = self.maxpool2(x2)\n",
    "        x2 = x2.squeeze(2)\n",
    "        \n",
    "        x3 = self.conv3(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "        x3 = self.ReLU_activation(x3)\n",
    "        x3 = self.maxpool3(x3)\n",
    "        x3 = x3.squeeze(2)\n",
    "\n",
    "#         x4 = self.conv4(CNN_x)#.squeeze(2)  # shape = (64, num_channels, 1)(squeeze 2)\n",
    "#         x4 = self.ReLU_activation(x4)\n",
    "#         x4 = self.maxpool4(x4)\n",
    "#         x4 = x4.squeeze(2)        \n",
    "        \n",
    "        all_out = torch.cat((x1, x2, x3), dim=1)\n",
    "        \n",
    "        all_out = all_out.unsqueeze(2)\n",
    "#         print('all_out: ',all_out.shape)\n",
    "        all_out = all_out.permute(2, 0, 1) \n",
    "#         print('all_out: ',all_out.shape)\n",
    "        \n",
    "        attn_output, attn_output_weights = self.self_attention(all_out, all_out, all_out)\n",
    "#         print('attn_output: ',attn_output.shape)\n",
    "        attn_output = attn_output.permute(1, 2, 0)\n",
    "#         print('attn_output2: ',attn_output.shape)\n",
    "        \n",
    "        attn_output = attn_output.squeeze(2)\n",
    "        \n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(attn_output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        CNN_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        \n",
    "        #combine two result\n",
    "        final_input = torch.cat((LR_result, CNN_result), dim=1)\n",
    "#         print(final_input.shape)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))        \n",
    "        \n",
    "        return result,LR_result,CNN_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_BiGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_BiGRU, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "                \n",
    "        self.bi_gru = nn.GRU(input_size = feature_size2, hidden_size = hidden_dim//2 , num_layers=num_of_lstm_layer,  bidirectional=True)\n",
    "        \n",
    "        \n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        #GRU part\n",
    "        \n",
    "        GRU_x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        GRU_x = GRU_x.permute(1, 0, 2)  # Batch_size * (feature_dim) * max_sen_len\n",
    "        bigru_out, _ = self.bi_gru(GRU_x)        \n",
    "        output = bigru_out[-1]      \n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        GRU_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        \n",
    "        #combine two result\n",
    "        final_input = torch.cat((LR_result, GRU_result), dim=1)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))        \n",
    "        \n",
    "        return result,LR_result,GRU_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_single_direc_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_single_direc_GRU, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "                \n",
    "        self.bi_gru = nn.GRU(input_size = feature_size2, hidden_size = hidden_dim , num_layers=num_of_lstm_layer,  bidirectional=False)\n",
    "        \n",
    "        \n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        #GRU part\n",
    "        \n",
    "        GRU_x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        GRU_x = GRU_x.permute(1, 0, 2)  # Batch_size * (feature_dim) * max_sen_len\n",
    "        bigru_out, _ = self.bi_gru(GRU_x)        \n",
    "        output = bigru_out[-1]      \n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        GRU_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        #combine two result\n",
    "        final_input = torch.cat((LR_result, GRU_result), dim=1)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))        \n",
    "        \n",
    "        return result,LR_result,GRU_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_BiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_BiLSTM, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "                \n",
    "        self.bilstm = nn.LSTM(input_size = feature_size2, hidden_size = hidden_dim , num_layers=num_of_lstm_layer,  bidirectional=True)\n",
    "        \n",
    "        \n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        #GRU part\n",
    "        \n",
    "        LSTM_x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        LSTM_x = LSTM_x.permute(1, 0, 2)  # Batch_size * (feature_dim) * max_sen_len\n",
    "        bilstm_out, _ = self.bilstm(LSTM_x)        \n",
    "        output = bilstm_out[-1]      \n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        LSTM_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        \n",
    "        #combine two result\n",
    "        final_input = torch.cat((LR_result, LSTM_result), dim=1)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))        \n",
    "        \n",
    "        return result,LR_result,LSTM_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_single_direc_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_single_direc_LSTM, self).__init__()  \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "                \n",
    "        self.bilstm = nn.LSTM(input_size = feature_size2, hidden_size = hidden_dim , num_layers=num_of_lstm_layer,  bidirectional=False)\n",
    "        \n",
    "        \n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()   \n",
    "        \n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        #GRU part\n",
    "        \n",
    "        LSTM_x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        LSTM_x = LSTM_x.permute(1, 0, 2)  # Batch_size * (feature_dim) * max_sen_len\n",
    "        bilstm_out, _ = self.bilstm(LSTM_x)        \n",
    "        output = bilstm_out[-1]      \n",
    "        \n",
    "        info_fusion = self.tanh_activation(self.fc1(output))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion)) \n",
    "        final_out = self.fc3(info_fusion)\n",
    "        LSTM_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        \n",
    "        #combine two result\n",
    "        final_input = torch.cat((LR_result, LSTM_result), dim=1)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))        \n",
    "        \n",
    "        return result,LR_result,LSTM_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LR_MLP, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.course_embedding = torch.nn.Embedding(nb_courses, course_emb_size)\n",
    "        self.video_embedding = torch.nn.Embedding(nb_videos, video_emb_size)\n",
    "        \n",
    "        self.ReLU_activation =  nn.ReLU()\n",
    "        self.tanh_activation =  nn.Tanh()\n",
    "        self.sigmoid_activation = nn.Sigmoid()\n",
    "        \n",
    "        self.lr_fc = nn.Linear(feature_size1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(feature_size2*sequence_len, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        self.final_fc = nn.Linear(2, 1)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, courseid_LR,courseid_CNN,continuesfeature1,continuesfeature2,videoid,b_size):\n",
    "        \n",
    "        #course_id  (batch_size, max_sen_len)\n",
    "        #continues  (batch_size, max_sen_len, feature_size)\n",
    "        emb1_LR = self.course_embedding(courseid_LR)\n",
    "        emb1_CNN = self.course_embedding(courseid_CNN)# (batch_size,max_sen_len, embed_size)\n",
    "        emb2 = self.video_embedding(videoid)\n",
    "        \n",
    "        # LR part\n",
    "        LR_x = torch.cat([emb1_LR,continuesfeature1], 1)\n",
    "        LR_result = self.sigmoid_activation(self.lr_fc(LR_x))\n",
    "        \n",
    "        \n",
    "        #MLP part\n",
    "        x = torch.cat([emb1_CNN,emb2,continuesfeature2], 2)\n",
    "        \n",
    "        input_x = x.view(b_size,-1)   \n",
    "        info_fusion = self.tanh_activation(self.fc1(input_x))\n",
    "        info_fusion = self.tanh_activation(self.fc2(info_fusion))\n",
    "        final_out = self.fc3(info_fusion)\n",
    "        MLP_result = self.sigmoid_activation(final_out)\n",
    "        \n",
    "        #combine two results\n",
    "        final_input = torch.cat((LR_result, MLP_result), dim=1)\n",
    "        result = self.sigmoid_activation(self.final_fc(final_input))      \n",
    "\n",
    "        return  result,LR_result,MLP_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (course_embedding): Embedding(707, 5)\n",
       "  (fc): Linear(in_features=47, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_MLP = MLP()\n",
    "# model_MLP.load_state_dict(torch.load('mlp_25epoch.model'))\n",
    "# model_MLP.eval()\n",
    "\n",
    "# model_GRU = GRU()\n",
    "# model_GRU.load_state_dict(torch.load('single_direc_gru_25epoch.model'))\n",
    "# model_GRU.eval()\n",
    "\n",
    "model_BiGRU = BiGRU()\n",
    "model_BiGRU.load_state_dict(torch.load('gru_25epoch.model'))\n",
    "model_BiGRU.eval()\n",
    "\n",
    "# # model_BiGRU_MultiAttent = BiGRU_MultiAttent()\n",
    "# # model_BiGRU_MultiAttent.load_state_dict(torch.load('gru_MultiHeadAtten_30epoch.model'))\n",
    "# # model_BiGRU_MultiAttent.eval()\n",
    "\n",
    "model_LSTM = LSTM()\n",
    "model_LSTM.load_state_dict(torch.load('single_direc_lstm_25epoch.model'))\n",
    "model_LSTM.eval()\n",
    "\n",
    "model_BiLSTM = BiLSTM()\n",
    "model_BiLSTM.load_state_dict(torch.load('Bilstm_25epoch.model'))\n",
    "model_BiLSTM.eval()\n",
    "\n",
    "model_LR = LR()\n",
    "model_LR.load_state_dict(torch.load('lr_40epoch.model'))\n",
    "model_LR.eval()\n",
    "\n",
    "# model_TextCNN = TextCNN()\n",
    "# model_TextCNN.load_state_dict(torch.load('textcnn_25epoch.model'))\n",
    "# model_TextCNN.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_single_model(course_vecs_LR,course_vecs_CNN):\n",
    "    course_id = []\n",
    "    video_id = []\n",
    "    continues_feature = []\n",
    "     \n",
    "    \n",
    "    #get x for LR\n",
    "    course_info_LR = course_vecs_LR\n",
    "\n",
    "    course_id_LR = []\n",
    "    continues_feature1 = []\n",
    "    for i in range(len(course_info_LR)): #get a course\n",
    "        c = course_info_LR[i]\n",
    "        cat_feture1 = c[0]       #get course_id\n",
    "        con_feture = c[1:]        #get continues features\n",
    "        course_id_LR.append(cat_feture1) \n",
    "        continues_feature1.append(con_feture)\n",
    "        \n",
    "        \n",
    "    #get x for CNN\n",
    "    course_list = course_vecs_CNN\n",
    "    course_id_CNN = []\n",
    "    video_id = []\n",
    "    continues_feature2 = []\n",
    "\n",
    "    for i in range(len(course_list)): #get a course \n",
    "        c = course_list[i]\n",
    "        course_cat1 = []\n",
    "        course_cat2 = []\n",
    "        course_con = []\n",
    "        for j in range(len(c)):       #get a subject\n",
    "            s = c[j]\n",
    "            cat_feture1 = s[0]       #get course_id and video_id\n",
    "            cat_feture2 = s[1]\n",
    "            course_cat1.append(cat_feture1)\n",
    "            course_cat2.append(cat_feture2)\n",
    "            con_feture = s[2:]        #get continues features\n",
    "            course_con.append(con_feture)\n",
    "        if len(course_cat1)<sequence_len:\n",
    "            length = sequence_len - len(course_cat1)\n",
    "            temp_course_id = [706] * length\n",
    "            temp_video_id = [38180] * length\n",
    "            temp2 = [[0,0,0,0,0,0,0,0,0,0,0,0,0]] * length\n",
    "            course_cat1 = course_cat1 + temp_course_id\n",
    "            course_cat2 = course_cat2 + temp_video_id\n",
    "            course_con = course_con + temp2\n",
    "\n",
    "        course_id_CNN.append(course_cat1) \n",
    "        video_id.append(course_cat2) \n",
    "        continues_feature2.append(course_con)\n",
    "\n",
    "    # to tensor\n",
    "    continues_feature1 = torch.tensor(continues_feature1)\n",
    "    course_id_LR = torch.tensor(course_id_LR).clone().long()\n",
    "    \n",
    "    \n",
    "    continues_feature2 = torch.tensor(continues_feature2)\n",
    "    course_id_CNN = torch.tensor(course_id_CNN).clone().long()\n",
    "    video_id = torch.tensor(video_id).clone().long()\n",
    "    \n",
    "    \n",
    "    predictions_gru = model_LSTM(course_id_CNN,video_id,continues_feature2)\n",
    "#     predictions_gru = model_MLP(course_id_CNN,video_id,continues_feature2,len(course_id_LR))\n",
    "\n",
    "#     predictions_lr = model_LR(course_id_LR,continues_feature1)\n",
    "    \n",
    "    predictions = torch.flatten(predictions_gru)\n",
    "    predictions_round = torch.round(predictions)\n",
    "    results = predictions_round.detach().numpy().tolist()\n",
    "    results = list(map(int,results))\n",
    "    \n",
    "    results_prob = predictions.detach().numpy().tolist()\n",
    "#     result_prob = list(map(int,result_prob))\n",
    "    \n",
    "\n",
    "    return results, results_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_train_seperately(course_vecs_LR,course_vecs_CNN):\n",
    "    course_id = []\n",
    "    video_id = []\n",
    "    continues_feature = []\n",
    "     \n",
    "    \n",
    "    #get x for LR\n",
    "    course_info_LR = course_vecs_LR\n",
    "\n",
    "    course_id_LR = []\n",
    "    continues_feature1 = []\n",
    "    for i in range(len(course_info_LR)): #get a course\n",
    "        c = course_info_LR[i]\n",
    "        cat_feture1 = c[0]       #get course_id\n",
    "        con_feture = c[1:]        #get continues features\n",
    "        course_id_LR.append(cat_feture1) \n",
    "        continues_feature1.append(con_feture)\n",
    "        \n",
    "        \n",
    "    #get x for CNN\n",
    "    course_list = course_vecs_CNN\n",
    "    course_id_CNN = []\n",
    "    video_id = []\n",
    "    continues_feature2 = []\n",
    "\n",
    "    for i in range(len(course_list)): #get a course \n",
    "        c = course_list[i]\n",
    "        course_cat1 = []\n",
    "        course_cat2 = []\n",
    "        course_con = []\n",
    "        for j in range(len(c)):       #get a subject\n",
    "            s = c[j]\n",
    "            cat_feture1 = s[0]       #get course_id and video_id\n",
    "            cat_feture2 = s[1]\n",
    "            course_cat1.append(cat_feture1)\n",
    "            course_cat2.append(cat_feture2)\n",
    "            con_feture = s[2:]        #get continues features\n",
    "            course_con.append(con_feture)\n",
    "        if len(course_cat1)<sequence_len:\n",
    "            length = sequence_len - len(course_cat1)\n",
    "            temp_course_id = [706] * length\n",
    "            temp_video_id = [38180] * length\n",
    "            temp2 = [[0,0,0,0,0,0,0,0,0,0,0,0,0]] * length\n",
    "            course_cat1 = course_cat1 + temp_course_id\n",
    "            course_cat2 = course_cat2 + temp_video_id\n",
    "            course_con = course_con + temp2\n",
    "\n",
    "        course_id_CNN.append(course_cat1) \n",
    "        video_id.append(course_cat2) \n",
    "        continues_feature2.append(course_con)\n",
    "\n",
    "    # to tensor\n",
    "    continues_feature1 = torch.tensor(continues_feature1)\n",
    "    course_id_LR = torch.tensor(course_id_LR).clone().long()\n",
    "    \n",
    "    \n",
    "    continues_feature2 = torch.tensor(continues_feature2)\n",
    "    course_id_CNN = torch.tensor(course_id_CNN).clone().long()\n",
    "    video_id = torch.tensor(video_id).clone().long()\n",
    "    \n",
    "    \n",
    "    predictions_gru = model_BiGRU(course_id_CNN,video_id,continues_feature2)\n",
    "#     predictions_gru = model_MLP(course_id_CNN,video_id,continues_feature2,len(course_id_LR))\n",
    "    predictions_lr = model_LR(course_id_LR,continues_feature1)\n",
    "    predictions = torch.flatten((predictions_gru+predictions_lr)/2)\n",
    "    predictions_round = torch.round(predictions)\n",
    "    results = predictions_round.detach().numpy().tolist()\n",
    "    results = list(map(int,results))\n",
    "    \n",
    "    results_prob = predictions.detach().numpy().tolist()\n",
    "#     result_prob = list(map(int,result_prob))\n",
    "    \n",
    "\n",
    "    return results, results_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_train_jointly(course_vecs_LR,course_vecs_CNN):\n",
    "    course_id = []\n",
    "    video_id = []\n",
    "    continues_feature = []\n",
    "     \n",
    "    \n",
    "    #get x for LR\n",
    "    course_info_LR = course_vecs_LR\n",
    "\n",
    "    course_id_LR = []\n",
    "    continues_feature1 = []\n",
    "    for i in range(len(course_info_LR)): #get a course\n",
    "        c = course_info_LR[i]\n",
    "        cat_feture1 = c[0]       #get course_id\n",
    "        con_feture = c[1:]        #get continues features\n",
    "        course_id_LR.append(cat_feture1) \n",
    "        continues_feature1.append(con_feture)\n",
    "        \n",
    "        \n",
    "    #get x for CNN\n",
    "    course_list = course_vecs_CNN\n",
    "    course_id_CNN = []\n",
    "    video_id = []\n",
    "    continues_feature2 = []\n",
    "\n",
    "    for i in range(len(course_list)): #get a course \n",
    "        c = course_list[i]\n",
    "        course_cat1 = []\n",
    "        course_cat2 = []\n",
    "        course_con = []\n",
    "        for j in range(len(c)):       #get a subject\n",
    "            s = c[j]\n",
    "            cat_feture1 = s[0]       #get course_id and video_id\n",
    "            cat_feture2 = s[1]\n",
    "            course_cat1.append(cat_feture1)\n",
    "            course_cat2.append(cat_feture2)\n",
    "            con_feture = s[2:]        #get continues features\n",
    "            course_con.append(con_feture)\n",
    "        if len(course_cat1)<sequence_len:\n",
    "            length = sequence_len - len(course_cat1)\n",
    "            temp_course_id = [706] * length\n",
    "            temp_video_id = [38180] * length\n",
    "            temp2 = [[0,0,0,0,0,0,0,0,0,0,0,0,0]] * length\n",
    "            course_cat1 = course_cat1 + temp_course_id\n",
    "            course_cat2 = course_cat2 + temp_video_id\n",
    "            course_con = course_con + temp2\n",
    "\n",
    "        course_id_CNN.append(course_cat1) \n",
    "        video_id.append(course_cat2) \n",
    "        continues_feature2.append(course_con)\n",
    "\n",
    "    # to tensor\n",
    "    continues_feature1 = torch.tensor(continues_feature1)\n",
    "    course_id_LR = torch.tensor(course_id_LR).clone().long()\n",
    "    \n",
    "    \n",
    "    continues_feature2 = torch.tensor(continues_feature2)\n",
    "    course_id_CNN = torch.tensor(course_id_CNN).clone().long()\n",
    "    video_id = torch.tensor(video_id).clone().long()\n",
    "    \n",
    "    \n",
    "    predictions,_,_ = model_LR_CNN_atten(course_id_LR,course_id_CNN,continues_feature1,continues_feature2,video_id)\n",
    "#     predictions,_,_ = model_LR_MLP(course_id_LR,course_id_CNN,continues_feature1,continues_feature2,video_id,len(course_id_LR))\n",
    "    predictions_round = torch.round(torch.flatten(predictions))\n",
    "    results = predictions_round.detach().numpy().tolist()\n",
    "    results = list(map(int,results))\n",
    "    results_prob = torch.flatten(predictions).detach().numpy().tolist()\n",
    "#     predictions,_,_ = model(course_id_LR,course_id_CNN,continues_feature1,continues_feature2,video_id)\n",
    "#     predictions = torch.round(torch.flatten(predictions))\n",
    "#     results = predictions.detach().numpy().tolist()\n",
    "#     results = list(map(int,results))\n",
    "    return results, results_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7296/7296 [02:02<00:00, 59.50it/s]\n"
     ]
    }
   ],
   "source": [
    "user_video_act_train_2_encoded_course_vec['predictions'] = user_video_act_train_2_encoded_course_vec.progress_apply(lambda row: prediction_train_seperately(row['course_vecs_LR'],row['course_vecs_CNN']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8284921550813956"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_result = user_video_act_train_2_encoded_course_vec[['predictions']].values.tolist()\n",
    "# ground_truth = user_video_act_train_2_encoded_course_vec[['label_list']].values.tolist()\n",
    "# # test = final_result.values.tolist()\n",
    "# final_result = [ item for elem in final_result for item in elem]\n",
    "# final_result = [ item for elem in final_result for item in elem]\n",
    "# ground_truth = [ item for elem in ground_truth for item in elem]\n",
    "# ground_truth = [ item for elem in ground_truth for item in elem]\n",
    "# acc = calculate_acc(final_result,ground_truth)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7296/7296 [02:10<00:00, 55.92it/s]\n"
     ]
    }
   ],
   "source": [
    "user_video_act_train_2_encoded_course_vec['predictions'],user_video_act_train_2_encoded_course_vec['predictions_prob'] =zip(*user_video_act_train_2_encoded_course_vec.progress_apply(lambda row: prediction_train_seperately(row['course_vecs_LR'],row['course_vecs_CNN']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7296/7296 [00:39<00:00, 184.43it/s]\n"
     ]
    }
   ],
   "source": [
    "user_video_act_train_2_encoded_course_vec['predictions'],user_video_act_train_2_encoded_course_vec['predictions_prob'] =zip(*user_video_act_train_2_encoded_course_vec.progress_apply(lambda row: prediction_train_jointly(row['course_vecs_LR'],row['course_vecs_CNN']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7296/7296 [01:09<00:00, 104.45it/s]\n"
     ]
    }
   ],
   "source": [
    "user_video_act_train_2_encoded_course_vec['predictions'],user_video_act_train_2_encoded_course_vec['predictions_prob'] =zip(*user_video_act_train_2_encoded_course_vec.progress_apply(lambda row: prediction_single_model(row['course_vecs_LR'],row['course_vecs_CNN']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8219608613450536"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = user_video_act_train_2_encoded_course_vec[['predictions']].values.tolist()\n",
    "final_result_prob = user_video_act_train_2_encoded_course_vec[['predictions_prob']].values.tolist()\n",
    "ground_truth = user_video_act_train_2_encoded_course_vec[['label_list']].values.tolist()\n",
    "# test = final_result.values.tolist()\n",
    "final_result = [ item for elem in final_result for item in elem]\n",
    "final_result = [ item for elem in final_result for item in elem]\n",
    "final_result_prob = [ item for elem in final_result_prob for item in elem]\n",
    "final_result_prob = [ item for elem in final_result_prob for item in elem]\n",
    "ground_truth = [ item for elem in ground_truth for item in elem]\n",
    "ground_truth = [ item for elem in ground_truth for item in elem]\n",
    "acc = calculate_acc(final_result,ground_truth)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8914712172336592"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc = roc_auc_score(ground_truth, final_result_prob)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7693714799010429"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1=f1_score(ground_truth, final_result, average='macro')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
